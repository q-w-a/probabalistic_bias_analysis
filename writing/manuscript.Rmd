---
title: "Approximating the Number of COVID-19 Infections in the United States Across 2021"
author: "Quinn White"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: false
    number_sections: false
header-includes:
 \usepackage{float}
 \usepackage{amsmath}
bibliography: "`r rbbt::bbt_write_bib('bib/manuscript.bib', overwrite = TRUE)`"
csl: "csl/nature.csl"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,  message=FALSE,
                      warning=FALSE, fig.pos="H",
                      out.extra = "")
library(knitr)
library(here)

```


\newpage 

# Abstract


# Introduction


# Results

## State-level Estimates

At both the state and county levels, we considered multiple implementations of the probabilistic bias analysis to compare estimates produced using different specification of the priors using survey data from the COVID-19 Trends and Impact Survey [@salomon2021a].

However, for simplicity, at the state-level we focus on the implementation where the priors do not vary by state or date. This also allows us to consider the entirety of 2021, since survey data is only available for dates after March 20, 2021. A full comparison of implementations is included in Supplementary Figure covidestim-concordance-state. 

In Figure \@ref(fig:ratio-estimated-to-observed-by-wave), we consider three distinct two-week intervals during waves of the pandemic in 2021. 

Although incidence of COVID-19 was highest during the time interval during the Omicron wave, as is clear in Figure  \@ref(fig:simulation-intervals-state),  we see that the ratio of estimated infections to observed infections is higher during the time intervals in the alpha and delta waves. This distinction is explained by the differences in testing rates during these period: on average, the testing rate during this two-week interval during the omicron wave was 2.4 times that of the alpha wave and 4.9 times that of the delta wave. This particular time interval during the delta wave; June 16, 2021 through July 1st, 2021; had the maximum ratio of estimated to observed infections, and corresponded to the minimum testing rate (Supplementary Figure testrate-low-summer).

Several states consistently have among the highest or lowest ratios of estimated to observed infections. In particular, there are 6 states with among the lowest 10 ratios of estimated infections to observed infections, and as such the highest case ascertainment rates, for more than 80% of time intervals considered. These states were Rhode Island, Massachusetts, District of Columbia, Alaska, New York, and Vermont. Meanwhile, states that had the highest ratios, and equivalently the lowest case ascertainment rates, include Mississippi, South Dakota, Oklahoma, Nebraska, and Tennessee.


```{r ratio-estimated-to-observed-by-wave, fig.cap="The ratio of estimated infections to observed infections for three time intervals of interest: one during the alpha wave, one during the delta wave, and one during the omicron wave. Although the prevalence of COVID-19 was highest during the omicron wave, the ratios of estimated to observed infections are higher for the time intervals during the alpha and delta waves, a difference that was driven by lower testing rates during these times. The trend we see in these three time intervals where Mississippi, South Dakota, Oklahoma, Nebraska, and Tennessee have among the highest ratios of estimated infections to observed infections, and as such the lowest case ascertainment rates, is consistent across the full set of time intervals considered from January of 2021 to March of 2022."}

include_graphics(here('figures/ratio-estimated-to-observed-multiple-waves.pdf'))

```


### Comparison to Covidestim

Because there is no established ground truth for the true number of infections for any time-interval, a useful source of comparison is other models that also estimate the true number of incident infections. Covidestim is a notable Bayesian nowcasting model that has been maintained throughout the course of the pandemic, sharing publicly available estimates at the state and county levels [@chitwood2022]. Consequently, we compare our estimates to Covidestim, with recognition that the Covidestim model is not a ground truth, and this comparison serves primarily to gain insight into the times and locations where estimates are concordant or discordant rather than to validate the approach. Given the true number of infections is unobserved, we are to some extent operating in the unknown, and must rely on assumptions based on available data. 

In Figure \@ref(fig:simulation-intervals-state), we see the simulation intervals for all two-week intervals and all states as well as the 95% Covidestim credible intervals summed to be on the same time scale. 
We see agreement is much higher before the time period spanning December 2021 through January 2022, where Covidestim intervals tend to be higher than the probabilistic bias intervals. This period corresponds to the Omicron wave of the pandemic (Supplementary Figure michigan-variant). This was a major shift in the pandemic: the Omicron variant is highly transmissible compared to previous variants, and it has immune invasion capacity, which means vaccines provides less protection against infection [@andrews2022]. Omicron also is associated with a lower infection fatality ratio [@liu2022], as well as milder infection overall, which may influence testing behavior. Its rise to dominance was rapid, presumably as a result of this enhanced transmissibility and immune escape;  it became the dominant variant over the course of a single month. 

As a result of these dramatic changes, Chitwood *et al.* @chitwood2022 made substantial changes to Covidestim, as noted in the model change log. Because the variant causes much milder infections, the infection fatality for Omicron infections is lower than previous variants, and death counts were much lower relative to the number of infections. To handle this change, rather than fitting model with deaths, they switched to using hospitalizations. They also allowed for the possibility of reinfections, since although reinfections were more rare with previous variants, Omicron is associated with higher reinfection rates [@pulliam2022]. The changes in the model may contribute to the differences we see between the probabilistic bias intervals and Covidestim intervals during the Omicron wave. 

Another difference we see is in the months of the summer of 2021, where the increase in Covidestim estimates appears lagged in comparison to the probabilistic bias simulation intervals. The difference we see between the Covidestim estimates and probabilistic bias intervals is likely a result of the way these approaches treat incomplete testing. The focus of the probabilistic bias analysis is to correct for incomplete testing, and as such the method is sensitive to changes in the total number tested and the positivity rate (see Figure DIAGRAM in the methods section). By contrast, while Covidestim models probabilities of diagnosis by symptom state to allow for variation in case ascertainment, the total number of tests is not an input into the model, so model estimates are not affected by changes in testing rate. This is particularly relevant during the summer of 2021, when an increase in test positivity preceded the increase in observed tests, leading to an increase in the probabilistic bias simulation intervals that precedes the increase in Covidestim estimates in many states (Supplementary Figure lagged-figs). Differences attributed to how these methods consider test positivity are likely to be heightened during times where increases in infections outpace testing capacity or test utilization.  


```{r simulation-intervals-state, fig.cap="Simulation intervals for each 2-week interval considered, for all states. For any given state, each vertical bar shows the 2.5\\% percentile and 97.5\\% percentile for the total number of infections in that two-week interval. Covidestim intervals summed over the same two-week time-scale are shown in red. The scale on the $y$-axis is distinct across states to highlight differences across time within each state.", fig.height=10,fig.width=14}

include_graphics(here('figures/intervals-and-covidestim-all-states.pdf'))

```


## County Level Estimates in Massachusetts

At the county level, this approach to approximating the true number of infections is only possible in a subset of the counties in the United States due to the need for both positive tests and total tests. While positive tests are frequently reported at the county level, total tests are not. 

We focus on Massachussetts because this state is the only state that reports both positive and total tests at the county-level and has wastewater data consistently reported throughout the entire time period of interest. Wastewater data here is useful in the sense that it is inherently not subject to the same biases as voluntary testing data, since it captures anyone in the wastewater catchment area, and as such is not influenced by more symptomatic people being more likely to be tested or differences in access to testing.

In Figure \@ref(fig:sim-intervals-color-county), we present simulation intervals for two of the four implementations of probabilistic bias analysis. For most time intervals, the other two implementations fall between those shown in the figure.

In the three out of the four implementations, the prior distributions specified for the bias parameters vary by state and date. In particular, we use survey data from the COVID-19 Trends and Impact Survey [@salomon2021a] to inform the prior distributions for one to two of the bias parameters: the probability of having symptoms among the untested population ($\Pr(S_1|\text{untested})$), and the parameter $\beta$, which represents the ratio of the test positivity among members of the untested population who have no symptoms to the overall test positivity. More detail on these implementations is provided in the [Methods](#Methods) section. In another, more simple, implementation, which is the only implementation presented at the state-level (Figure \@ref(fig:ratio-estimated-to-observed-by-wave) and \@ref(fig:simulation-intervals-state)), the prior distributions are informed by the distributions from the survey data across all dates and states, but the same prior distributions are used for all states and time intervals considered.

While there is substantial overlap among all of the versions, the implementation allowing $\beta$ and $\Pr(S_1|\text{untested})$ to vary by state and date tends to be the highest, followed by that only allowing $\beta$ to vary by state and date,  followed by that only allowing $\Pr(S_1|\text{untested})$ to vary by state and date, and lastly the implementation that does not vary the priors by state or date tends to produce the lowest estimates (Supplementary Figure sim-intervals-faceted-county).

```{r sim-intervals-faceted-county, fig.width=15, fig.height=16,  fig.cap = "Simulation intervals for counties in Massachusetts. Each interval corresponds to a 95\\% simulation interval for the total number of estimated infections for that county in that two-week time interval. The columns represent different implementations of the probabilistic bias analysis: in the first through the third columns, the priors vary by state and date, while for the fourth, they are the same for all states and time intervals considered. For the implementation in the first column, we center the distribution of $\\beta$ at the ratio of the screening test positivity to the overall test positivity from the survey, and we center the distribution of $\\Pr(S_1|\\text{untested})$ at the percentage of the population experiencing COVID-19-like illness from the survey for each two-week interval. The second column centers only $\\beta$ at the aformentioned value, and the third column only centers $\\Pr(S_1|\\text{untested})$ at the aforementioned value. The fourth column corresponds to the implementation where we specify priors that are the same for all dates. The implementation that centers both $\\Pr(S_1|\\text{untested})$ and $\\beta$ at the survey values is consistently the highest among the implementations, followed by the implementation that centers only $\\beta$ at the survey value, followed then by the implementation only centering $\\Pr(S_1|\\text{untested})$ at the survey value, and then the lowest among the implementations is that where the priors do not vary by date.", include=FALSE}

include_graphics(here('figures/simulation-intervals-faceted-version-county.pdf'))

```


```{r sim-intervals-color-county, fig.width=15,  fig.cap = "Simulation intervals for counties in Massachusetts, colored by the implementation of probabilistic bias analysis. Only the two implementations that were consistently the highest and lowest among the implementations are included for clarity. In the   In the three out of the four implementations, the priors vary by state and date, while for the fourth, they are the same for all states and time intervals considered. For the first implementation, we center the distribution of $\\beta$ at the ratio of the screening test positivity to the overall test positivity from the survey, and we center the distribution of $\\Pr(S_1|\\text{untested})$ at the percentage of the population experiencing COVID-19-like illness from the survey for each two-week interval. The second implementation, we center only $\\beta$ at the aformentioned value, and the for the next we only center $\\Pr(S_1|\\text{untested})$ at the aforementioned value. The last implementation corresponds to the implementation where we specify priors that are the same for all dates. The implementation that centers both $\\Pr(S_1|\\text{untested})$ and $\\beta$ at the survey values is consistently the highest among the implementations, followed by the implementation that centers only $\\beta$ at the survey value, followed then by the implementation only centering $\\Pr(S_1|\\text{untested})$ at the survey value, and then the lowest among the implementations is that where the priors do not vary by date."}

include_graphics(here('figures/simulation-intervals-color-version-only-2-county.pdf'))

```

Also of interest beyond the total numbers of infections is the ratio of the infections that are estimated to those that were observed, as this gives us a sense for the case ascertainment at different time periods in the pandemic. We see in Figure \@ref(fig:ma-heatmap) that the two-week interval estimated to have the highest ratio of estimated to observed infections was July 2, 2021 through July 30, 2021, during the delta wave. This corresponds to a similar time frame that we saw at the state-level in Figure \@ref(fig:ratio-estimated-to-observed-by-wave). Hampshire County and Suffolk County were consistently among the lowest with regard to the ratio of estimated to observed infections, which may be explained by the screening testing occurring at universities in these counties.


```{r ma-heatmap, fig.cap=" The ratio of estimated to observed infections across time for counties in Massachusetts. Counties are ordered by the median ratio across time intervals, from the highest ratio (Barnstable) to the lowest (Hampshire). Similar to what we see at the state level, the highest ratios were during the summer of 2021 during the Delta wave -- a period of decreased testing. The span of time with the highest ratio of estimated to observed infections was July 2, 2021 through July 30, 2021."}

include_graphics(here('figures/ma_county_heatmap.pdf'))


```


As we see in Figure \@ref(fig:testrate-v-ratio), the nature of the relationship between the testing rate and the ratio of estimated infections to observed infections depends on whether we allow $\beta$ and $\Pr(S_1|\text{untested})$ to vary by location and date. In particular, when we sample from the same priors for every correction (the first panel of Figure \@ref(fig:testrate-v-ratio), we see there is little variability in the relationship between the testing rate and median estimated infections, because the form of the correction is identical for each two-week interval and state considered. Allowing $\beta$ and/or $\Pr(S_1|\text{untested})$ to vary by time and location introduces additional variability in the relationship between the ratio of estimated infections to observed and testing rate.

The nonlinearity of the relationship between the testing rate and the ratio of estimated infections to observed infections is more clear when we consider how we calculate the positives among the untested population. To do this, we split up the population $N$ into the number tested, $N_{untested}$, and untested, $N_{tested}$, for that two-week interval. 

Denoting $N^*$ to be the number who would test positive for COVID-19 if they were tested, on the $y$-axis, the ratio of estimated infections to observed infections is approximately^[This isn't exactly the estimated infections, because for simplicity of notation we are not writing out the correction for test inaccuracy.]
$\frac{N^*_{\text{tested}} + N^*_{\text{untested}}}{N^*_{\text{tested}}},$ where we calculate $N^*_{\text{untested}}$ using the specified priors, and $N^*_{\text{untested}}$ is the observed positive tests.

On the $x$-axis, we have the number tested over the population size, $\dfrac{N_{\text{tested}}}{N}$. Thus, we see the trend in each panel where for small changes in testing rate when the testing rate is very low, the ratio of unobserved to unobserved is very high since $N^*_{\text{untested}}$ will be large relative  $N^*_{\text{tested}}$, that is, a large proportion of infections are going undetected. However, with higher testing rates,  $N^*_{\text{untested}}$ will be smaller relative  $N^*_{\text{tested}}$, and the ratio of estimated to observed infections nears one.


```{r testrate-v-ratio, fig.cap ="The ratio of the median estimated infections to observed infections plotted against the testing rate, where the testing rate is calculated as the total number tested in a two-week interval over the population size. When the priors are the same for all time intervals, there is minimal variability relationship between the testing rate and the ratio of estimated to observed infections, since the correction for incomplete testing and diagnostic test inaccuracy is identical for each time-interval. However, when we allow $\\beta$ or $\\Pr(S_1|\\text{untested})$ to vary over time, there is more variability in the relationship. A horizontal line in red at 1 is included to reference; a ratio of exactly 1 would indicate no infections went unobserved."}

include_graphics(here('figures/testing-rate-ratio-county.pdf'))

```

### Comparison to Wastewater Data and Covidestim Estimates

As with the state level results, we also compare estimates at the county-level to Covidestim estimates. However, at the county-level, we proceeded with Massachusetts specifically with the aim of providing another source of comparison: wastewater concentrations.

We see in Figure \@ref(fig:correlations) (a) that for all implementations, the probabilistic bias estimates are highly correlated with wastewater concentrations, and in Figure \@ref(fig:correlations) (b) we see they also highly correlated with the Covidestim estimates. The correlations in both cases are very similar between implementations, differing by less than 0.01. 


```{r correlations, fig.cap ="Considering the correlations between the probabilistic bias estimates and wastewater concentrations (a) and between probablistic bias estimates and Covidestim estimates (b). We see that all implementations considered are highly correlated with both wastewater concentrations and Covidestim estimates. In both panels, each point is a county-biweek, where the value on the $x$-axis is the median of the simulation interval for that county in that two-week interval. For (a), the value on the $y$-axis is the mean wastewater concentration for that county and two-week interval. For (b), the value on the $y$-axis is the median of the Covidestim credible interval for that county, summed to be on the same two-week-interval time scale. The correlations are highly similar between implementations, differing by less than 0.01."}

include_graphics(
  here('figures/correlations-wastewater-by-version.pdf')
  )


```


In Table \@ref(table:coverage), we summarize the concordance with Covidestim at the county and state levels by comparing the percent of simulation intervals where the Covidestim median falls above, below, or within the interval. At both geographic scales, we see that the probabilistic bias implementation that is most concordant with Covidestim is the that where the prior for $\Pr(S_1|\text{untested})$ is centered at the survey value of the percent of the population experiencing COVID-19-like illness, followed by the implementation that does not vary by state or date. This agreement was not a result of wider bounds, as the simulation intervals for the other two implementations were larger on average. For both of these implementations, when the Covidestim median was not contained, this was typically because it fell above the simulation interval. As is clear from Figure \@ref(fig:simulation-intervals-state) at the state-level, this often occurred during the Omicron wave, when the Covidestim estimates are consistently above the probabilistic bias counts. 


\renewcommand{\arraystretch}{2}

```{r}

library(tidyverse)
library(kableExtra)

county_coverage <- readRDS(here("figures/table_data/county_coverage.RDS"))
state_coverage <- readRDS(here("figures/table_data/state_coverage.RDS"))

row_num <- nrow(county_coverage) + nrow(state_coverage)

county_coverage %>%
  bind_rows(state_coverage) %>%
  group_by(`Geographic Scale`) %>%
  arrange(desc(`Percent Contained in Interval`),
          .by_group=TRUE) %>%
  mutate(across(contains("percent"),~ paste0(round(.,2), "\\%"))) %>%
  ungroup() %>%
  select(-`Geographic Scale`) %>%
  kbl(caption = "\\label{table:coverage}Coverage of Covidestim Medians at the County and State Levels",
      booktabs=TRUE,  
      escape=FALSE,
      digits=3) %>%
  kable_styling(latex_options=c("scale_down","HOLD_position"))  %>%
  group_rows(start_row=1, end_row =4, group_label= "County", 
             latex_gap_space = "0.7em",
             hline_after = TRUE) %>%
  group_rows(start_row=5, end_row =8, group_label= "State",
             latex_gap_space = "0.7em",
             hline_after = TRUE) %>%
  row_spec(1:(row_num-1),  hline_after = TRUE, 
           extra_css = "border-bottom: 1px solid;") %>%
  row_spec(0, font_size=11, bold=TRUE) %>%
  footnote(general = paste0("The percent of simulation intervals where the Covidestim median falls below, within, or above the interval,",
  "when considering all simulation intervals for that implementation and geographic scale. ",
  "Implementations are ordered from those with the highest proportion of Covidestim medians contained in the interval, to the lowest."),
  general_title ="", footnote_as_chunk=TRUE, threeparttable=TRUE)


```

Also of interest is whether there is a lag between the wastewater concentrations and bias corrected estimates. Although wastewater concentrations can be a leading indicator since people may shed viral material before developing symptoms and subsequently getting tested, the lead time is a result of multiple epidemiological factors, including viral shedding dynamics that may differ with evolution of the virus, access to testing, and testing behavior [@olesen2021a]. Indeed, the lead or lag time between wastewater concentrations and hospitalizations or cases has varies substantially between waves, where the lead time is higher in earlier waves and near zero at later waves, a difference that may be attributable to changes in diagnostic test availability over time  [@xiao2022a; @hopkins2023a].

In Figure \@ref(fig:cross-corr-wastewater), we see that among all counties except one in Massachsuetts, the maximum correlation between mean wastewater concentration and biweekly infection estimates was obtained at a lag of zero, that is, there  typically there was no lag among the biweekly infection estimates and mean wastewater concentrations. This is to be expected given the lead time for wastewater concentrations, when observed, is less than 2 weeks [@olesen2021a]. In two counties, Hampsphire and Barnstable, the lag between the probabilistic bias counts and wastewater concentration was less than that between the observed positive tests and wastewater concentrations.

```{r cross-corr-wastewater, fig.width=8, fig.cap="Results of cross correlation analysis between probabilistic bias estimates and wastewater concentrations aggregated to the county level for counties in Massachusetts. For each county, the maximum correlation between the probablistic bias estimates and wastewater concentration is shown, colored by implementation; also included is the correlation between the observed positive tests and wastewater concentration. The shape indicates the lag at which the maximum correlation occurred, with units in biweeks. For example, a lag of -1 indicates the maximum correlation is obtained when the biweekly concentrations are lagged by 1 biweek, or equivalently, that the wastewater concentrations lead the observed positive tests by one biweek. For all counties except Barnstable, the maximum correlations between wastewater concentrations and probablistic bias estimates occur at no lag. In Barnstable and Hampshire, observed tests lagged wastewater concentrations, but the probablistic bias estimates for these counties did not."}

include_graphics(here("figures/cross_correlation_observed_pb.pdf"))

```


# Discussion

### Limitations

- Validation:
  - Wastewater encatchments do not fall exactly on county lines, so aggregation here contains limited information
  - Covidestim -- sum of daily medians != median for entire two-week time interval
- Repeat testing may bias estimates in places where this form of testing comprises a substantial proportion of total testing, but data on people-viral-total and people-viral-positive is not widely available 


# Methods {#Methods}

## Data

We acquired state-level PCR testing data from public-use data on HealthData.gov, which includes both positive and negative test counts [@unitedstatesdepartmentofhealthandhumanservices]. At the county level, we obtained publicly available data from Massachusetts published on the state government website [@departmentofpublichealth].

Covidestim estimates for dates after December 2021 were obtained from the Covidestim API, and those  for dates after December 2021 were obtained from the legacy page. 

### State Level


### Massachusetts County Level


### Survey Data 

The COVID-19 Trends and Impact Survey was run in collaboration by ...


### Wastewater Data 

Biobot analytics ...

## Statistical Methods

```{r,out.width='70%', fig.align='center'}

include_graphics(here('figures/analysis_diagram.png'))

```


### Probabilistic bias analysis

### Bayesian Melding 

### Specification of Priors 

```{r melding-priors, fig.width=9,fig.height=9}

include_graphics(here('figures/melding-priors.pdf'))

```


#### $\alpha$


#### $\beta$ 


#### $\Pr(S_1| \text{untested})$

#### $\Pr(S_0|\text{test}_+,\text{untested})$ \newline\newline


\vspace{4 mm}

There is substantial heterogeneity in estimates of the percent of infections that are asymptomatic. This is in part due to distinct study populations and selection criteria. In particular, estimates from screening studies may be better estimates of the asymptomatic rate among the untested population: estimates from studies where the population was not screened, and as such was comprised of individuals that sought out a PCR test, may include a higher proportion of symptomatic individuals, biasing estimates of the asymptomatic rate downwards. 

One meta-analysis included studies across the globe as of February 4, 2021, and estimated the pooled percent of asymptomatic infections among confirmed infections to be 40.50% (95% CI 33.50%-47.50%) [@ma2021]. This analysis did not restrict to screening studies. Another meta-analysis, when restricting to screening studies, found the pooled asymptomatic percentage to be 47.3% (95% CI, 34.0 - 61.0%) [@sah2021]. Both meta-analyses noted the substantial amount of heterogeneity in the percent of asymptomatic infections.

In a large screening study where the sample was individuals arriving from overseas, the asymptomatic rate was 76.8% [@fang2023]. A screening study among children admitted to a pediatric emergency department between May 2020 and January 2021 found  the asymptomatic rate to be 51.7% [@ford2022]. 

Several studies were conducted among university students. Among students at the University of Arizona in the fall semester of 2020, including students who sought testing and who were required to test, the asymptomatic rate of infection was 79.2%. A study at the University of Notre Dame distinguished between presymptomatic infection and asymptomatic infection, and found found 32% to be asymptomatic throughout the entire course of infection, 27.0% to be presymptomatic, and 40.5% to be symptomatic.  The asymptomatic rate among nonresidential students participating in the surveillance testing system at Clemson University was 69%.The generalizability of these studies is limited given that students are typically healthy and as such may be more likely to experience asymptomatic infection.

Vaccine coverage also may influence the asymptomatic rate. In a study in Israel on the effectiveness of the Pfizer–BioNTech mRNA COVID-19 vaccine BNT162b2, 55.7% (49,138 out of 88203) of infections were asymptomatic in the unvaccinated group, and 68.2% of infections  (3632 out of 5324) of infections were asymptomatic in the vaccinated group. 

Numerous additional factors contribute to the heterogeneity we see among estimates of the percent of infections that are asymptomatic, including community prevalence, the study population, and the time period when the study population was tested. The use of different definitions also may contribute. This includes the definition of a symptomatic infection, since our understanding of the clinical presentation of COVID-19 has evolved over time [@sah2021], as well as the definition of asymptomatic, since some define asymptomatic to include presymptomatic cases, where people that had no symptoms upon testing positive but may have went on to develop symptoms at a later date, and truly asymptomatic cases, where an infected individual never goes on to develop symptoms. 

Because of the heterogeneity in estimates of the percent of infections that are asymptomatic, for this prior we specified a beta distribution with the majority of the density between 0.3 and 0.8, with a mean of 0.55 and standard deviation of 0.12.




# Data Availability 

# Code Availability 
  
# References

